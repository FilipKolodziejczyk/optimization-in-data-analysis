# Project

### Topic

DL5 (gr. 3) Learning neural networks using Stochastic Gradient Descent with tuning hyperpaarameters - AdaGrad method ([J. Duchi, et al, Adaptive subgradient methods for online learning and stochastic optimization. J Mach Learn Res. 2011;12, 2121-59]). Comparison with ADAM's version of SGD. Numerical experiments performed on datasets related to fine tuning of pretrained CollaboNet model aimed at Named Entity Recognition task ([W. Yoon, et al, CollaboNet: collaboration of deep neural networks for biomedical named entity recognition, BMC Bioinformatics 2019, 20(Suppl 10):249]). Project for 4 students.

### Requirements

- At least 3 meetings with the tutor (tutor must agree on the date beforehand)
- Analysis of the current state of knowledge in the field of the project
- Implementation based on the analysis of the current state of knowledge
- Free choice of data but must illustrate the effectiveness of the method
- Implemented method must be compared with the existing one
- [Report](#report-structure)

### Report structure

- a description of the current state of knowledge regarding the subject of the project
- description of the chosen optimization method to solve the optimization task given in the project topic
- description of the test data
- description of the implementation/use of the selected optimization method
- a description of the results of the numerical tests

### Grading

- evaluation of the project performance based on the presented project report and the program implementing the selected optimization method
- evaluation of the presentation (20 minutes long) presenting the results obtained in the project in front of other students participating in the course classes - during the presentation, the teacher asks questions to the members of the project team regarding the implementation of the project
- assessment of the implementation of the method during a meeting with the project team and assessment of oral answers to questions regarding the lecture material

### Papers and other materials

- [AdaGrad Paper](https://www.jmlr.org/papers/volume12/duchi11a/duchi11a.pdf)
- [AdaGrad Article with Code](https://machinelearningmastery.com/gradient-descent-with-adagrad-from-scratch/)
- [Adam Article](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)
- [CollaboNet GitHub](https://github.com/wonjininfo/CollaboNet)
- [Article about Named Entity Recognition](https://www.datacamp.com/blog/what-is-named-entity-recognition-ner)
